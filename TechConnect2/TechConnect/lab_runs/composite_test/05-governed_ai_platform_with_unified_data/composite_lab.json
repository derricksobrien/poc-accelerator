{
  "scenario": "Governed AI Platform with Unified Data",
  "composite_sources": [
    "unified-data-fabric"
  ],
  "source_repos": {
    "unified-data-fabric": {
      "solution_area": "Azure (Data & AI)",
      "complexity": "L400",
      "description": "Solution accelerator for SolutionAreaEnum.AZURE_DATA_AI"
    }
  },
  "integrated_architecture": {
    "components": [
      {
        "index": 1,
        "repo_id": "unified-data-fabric",
        "role": "Data & Analytics",
        "solution_area": "Azure (Data & AI)",
        "key_services": [
          "Fabric"
        ],
        "layer": "layer_0"
      }
    ],
    "data_flows": [],
    "diagram_ascii": "┌─────────────────────────────┐\n│ unified-data-fabric          │\n│ (Data & Analytics          ) │\n└─────────────────────────────┘",
    "integration_pattern": "Single Component"
  },
  "synthesized_instructions": "# Integrated Lab: Governed AI Platform with Unified Data\n\n## Overview\nThis lab combines 1 accelerators to build a complete solution:\n\n\n### 1. Unified Data Fabric\n**Solution Area:** SolutionAreaEnum.AZURE_DATA_AI  \n**Complexity:** ComplexityLevel.L400  \n**Role in Architecture:** Data & Analytics  \n\nSolution accelerator for SolutionAreaEnum.AZURE_DATA_AI\n\n\n## Integration Architecture\n\nThis lab follows a multi-layer architecture:\n\n1. **Data Layer**: Ingests and prepares data from source systems\n2. **Processing Layer**: Applies AI/ML transformations or analytics\n3. **Governance Layer**: Enforces compliance, auditing, and data protection policies\n4. **Application Layer**: Exposes insights to end users or downstream systems\n\n## Step-by-Step Instructions\n\n### Phase 1: Prerequisites & Setup\n- [ ] Create Azure subscription and required resources\n- [ ] Configure service principal for authentication\n- [ ] Set up local development environment with Python 3.10+\n- [ ] Clone all required repositories\n\n### Phase 2: Deploy Component 1 (unified-data-fabric)\n- [ ] Follow deployment guide in unified-data-fabric repository\n- [ ] Configure environment variables for Data & Analytics\n- [ ] Validate component is operational\n- [ ] Test core functionality\n\n\n### Phase Final: End-to-End Testing & Validation\n- [ ] Execute full data pipeline from source to destination\n- [ ] Verify all data flows match architecture diagram\n- [ ] Test governance policies and audit logging\n- [ ] Generate sample reports/dashboards\n- [ ] Document results and lessons learned\n\n## Data Flow\n\n\n## Success Criteria\n\n✅ All components deployed and operational  \n✅ Data flows successfully through entire pipeline  \n✅ Governance policies active and auditing enabled  \n✅ End-to-end test scenario completed successfully  \n✅ Performance meets baseline requirements  \n\n## Troubleshooting\n\nSee individual accelerator READMEs for component-specific troubleshooting.\n\n## Next Steps\n\nAfter completing this lab:\n1. Extend with additional data sources\n2. Customize business logic for your use case\n3. Scale to production with monitored deployments\n4. Integrate with existing enterprise systems\n",
  "deployment_steps": [
    {
      "sequence": 1,
      "phase": "Prerequisites",
      "title": "Create Azure resources and authentication",
      "description": "Set up service principals, resource groups, and access controls",
      "commands": [
        "az group create --name tech-connect --location eastus",
        "az identity create --name techconnect-identity --resource-group tech-connect"
      ],
      "estimated_minutes": 10,
      "depends_on": []
    },
    {
      "sequence": 2,
      "phase": "Deploy Component 1",
      "title": "Deploy unified-data-fabric",
      "description": "Follow unified-data-fabric deployment guide",
      "repo_source": "unified-data-fabric",
      "commands": [
        "cd repos/unified-data-fabric",
        "az deployment group create --template-file main.bicep --resource-group tech-connect"
      ],
      "estimated_minutes": 20,
      "depends_on": [
        1
      ]
    },
    {
      "sequence": 3,
      "phase": "Integration",
      "title": "Validate end-to-end data flow",
      "description": "Test pipeline from source through all components",
      "commands": [
        "python test_integration.py",
        "pytest --verbose"
      ],
      "estimated_minutes": 15,
      "depends_on": [
        2
      ]
    }
  ],
  "merged_assets": {
    "deployment_scripts": [
      {
        "repo": "unified-data-fabric",
        "script_name": "unified-data-fabric_deploy.sh",
        "execution_order": 1,
        "description": "Deploy unified-data-fabric component"
      }
    ],
    "configuration_templates": [
      {
        "repo": "unified-data-fabric",
        "template_type": "bicep",
        "file_name": "unified-data-fabric_deploy.bicep",
        "description": "IaC template for unified-data-fabric"
      }
    ],
    "sample_code_languages": [
      {
        "repo": "unified-data-fabric",
        "language": "python",
        "file": "main.py"
      },
      {
        "repo": "unified-data-fabric",
        "language": "typescript",
        "file": "index.ts"
      }
    ],
    "integration_points": []
  },
  "prerequisites": [
    "Azure CLI (version 2.40+)",
    "Azure Synapse or Fabric knowledge",
    "Azure subscription with Contributor role",
    "Basic understanding of LLMs and embeddings",
    "Familiarity with SQL queries",
    "Git (version 2.30+)",
    "GitHub account",
    "Knowledge of vector databases",
    "OpenAI API key or Azure OpenAI resource",
    "Python 3.10+",
    "Understanding of data warehousing concepts",
    "Visual Studio Code or IDE of choice"
  ],
  "estimated_duration_hours": 3.0,
  "responsible_ai_tag": false,
  "rai_disclaimer": null
}